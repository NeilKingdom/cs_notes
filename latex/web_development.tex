\documentclass{article}

\usepackage{titling}
\usepackage{geometry}
\usepackage{fontspec}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{tabularx}
\usepackage{fancyhdr}
\usepackage{tcolorbox}
\usepackage{minted}

\definecolor{bg}{RGB}{22,43,58}

\geometry{margin=1in}
\setmainfont{LiberationSans}

% C codeblocks
\tcbuselibrary{listings, minted, skins}
\tcbset{listing engine=minted}
\newtcblisting{cblk}{%
    listing only,
    minted language=c,
    minted style=monokai,
    colback=bg,
    enhanced,
    frame hidden,
    minted options={%
        tabsize=4,
        breaklines,
        autogobble
    }
}

% TS codeblocks
\tcbuselibrary{listings, minted, skins}
\tcbset{listing engine=minted}
\newtcblisting{tsblk}{%
    listing only,
    minted language=c,
    minted style=monokai,
    colback=bg,
    enhanced,
    frame hidden,
    minted options={%
        tabsize=4,
        breaklines,
        autogobble
    }
}

\hypersetup{%
    colorlinks=true,
    urlcolor=blue,
    linkcolor=gray
}

% Using fancyhdr to place page no. in bottom right corner
\fancyhf{}
\pagestyle{fancy}
\fancyfoot[R]{\thepage}
\renewcommand{\headrulewidth}{0pt}

\begin{document}

% Center title page vertically
\renewcommand\maketitlehooka{\vfill}
\renewcommand\maketitlehookd{\vfill}

\begin{titlingpage}
    \title{Web Development}
    \author{Neil Kingdom}
    \maketitle
\end{titlingpage}

\newpage

\tableofcontents

\newpage

\section{Abstract}

This document will focus on the evolution of web development since its inception. Of course, web dev is such
a massive topic that we won't have time to cover every aspect in robust detail. Rather the goal is to
demonstrate how we got to where we are now. New developers are sort of forced to jump into the deep end since
modern frameworks stand upon the shoulders of so many old technologies that have been lost to time. Exposing
these underlying technologies will hopefully help new devs see patterns and understand the ideas behind some
of the decisions made.

\section{Introduction}

As you're likely aware, the web is one of the quickest technologies to evolve and innovate. For programmers
who did not get the opportunity to follow these changes since the web's inception, it can be quite daunting to
try getting into web when it seems that their already so far behind. As most probably know, the web is quite a
recent phenomena, beginning in only the early 90's. We refer to this era as web 1.0. This was a time where
websites were written in pure HTML and CSS. There was no concept of JavaScript or any of these other fancy
technologies. It was plain and simple... Perhaps a little too plain and simple though. People could read blogs
and articles and the news during this time, but it was more of a cool toy rather than something taken seriously.
It was with the rise of things like MySpace and eventually Facebook where we entered web 2.0: a more interactive
era of the web where users could begin to input data and receive feedback. Some argue that we are entering into
the era of web 3.0 with AI, as well as WebAssembly, which will allow people to run extremely CPU intensive
applications natively in a web interface/browser.

\section{Revisiting Networking Protocols}

In order to get to where we are today, it's best, in my opinion, to go back to basics. If you haven't already
read my networking notes or have some knowledge of networking, I might recommend that as a precursor to this
document. Luckily, we don't need to understand too many protocols when it comes to web development. The most
important are by far DNS, TCP, HTTP, and HTTPS (which is a composition of HTTP and TLS - formerly known as SSL).
In case you're not familiar with the ackronyms, TCP stands for Transmission Control Protocol. It rests at the
transfer layer of both the OSI and TCP/IP models alongisde UDP, which stands for User Datagram Protocol. TCP
has been the defacto standard for transferring packets across the web, although this is changing with HTTP/3.
HTTP stands for HyperText Transfer Protocol and rests at the Application Layer in both the OSI and TCP/IP
models. HTTP is the standard for fetching or sending data related to the actual content that is rendered on the
user-side. HTTP/3 is the newest standard for HTTP and is beginning to use UDP rather than TCP. UDP is a much
less reliable protocol for sending data since network packets are not acknowledged when received. They may come
out of order or be dropped entirely. On the contrary, TCP packets are sent in the correct order and must be
acknowledged by the receiver, otherwise the sender must re-send the lost packet. HTTPS is just HTTP "Secure".
It uses the Transfer Layer Secure (TLS), formerly known as Secure Socket Layer (SSL), protocol on top of HTTP
to encrypt the contents of HTTP requests and responses. Virtually all of the web uses HTTPS these days. The
last really relevant protocol used in web is Domain Name System (DNS). We'll cover DNS in more detail, but know
that it's essentially how your browser is able to take a URL and convert it into an IP address that it can
find. Speaking of IP addresses, it is somewhat important to remember that your router uses Network Address
Translation (NAT) to convert your private IP address into a public facing IP address. On Linux, you can use the
ip command (probably already installed on your distro) to list out your PC's non-public facing IP address. You
can either run ip addr show, or abbreviated: ip a show. This will list the loopback, ethernet, and wireless
interfaces for your network card listed as something like lo, eth0, and wlan0, respectively. If not installed,
get the nmap command. Copy your PC's IP address alongside the port and run the following:
nmap -T4 xxx.xxx.xxx.xxx/xx. This will list all devices connected to your LAN. These should all share some
portion of your PC's IP address depending upon the subnet mask. Typically, the subnet mask defaults to
255.255.255.0, meaning that all IP addresses on your LAN will share the first three bytes xxx.xxx.xxx.nnn where
nnn is unique to your device. When any device on your network requests a resource such as a webpage, the
request is forwarded to your router. Your router/default gateway takes your PC's private IP address and
converts it to a public-facing IP address using a NAT Translation Table before sending it to your ISP to be
forwarded to the internet. There are websites such as showmyip.com that will tell you what your public-facing
IP address is.

\section{The HTTP Request and Response}

If we're to understand web, it's quite crucial to understand HTTP requests and responses. An HTTP request/
response is a protocol for HTTP servers to know how and what to fetch or send to the user. An HTTP request
adheres to the following format:

<Method> <Request-URI> <HTTP-Version> <CRLF>\\
\[Headers\] <CRLF>\\
\[Message-body\]

Let's break this down:

\begin{itemize}

\item{%
   \textbf{Method:} HTTP defines certain request methods which are REST compliant. We'll talk more
   about REST later. These methods include GET, POST, PUT, DELETE (these are the most common ones), HEAD,
   CONNECT, OPTIONS, TRACE, and PATCH. PUT and POST are often confused with one another. Essentially POST is
   used to create a new resource, whereas PUT is used to create or update a resource. PUT is known as an
   idempotent request, meaning non-destructive, whereas POST is non-idempotent, meaning that it will
   overwrite previous data. For example, if creating a button that increments a shopping cart count, you'd
   want request to be a POST request to update the count, otherwise it would remain the same if using PUT.
   HEAD is like GET but omits the response body.
}

\item{%
   \textbf{Request-URI:} The Request-URI is a Uniform Resource Identifer (URI) that is used to
   identify a name or resource on the internet. This differs from a Uniform Resource Locator (URL), which is a
   subset of URIs and specifies where the resource is available and the mechanism for retrieving it. A URI can
   be made up of the following components:

   \begin{itemize}

   \item{%
      \textbf{Scheme/Protocol:} This is the protocol used for fetching the resource e.g. HTTP
   }

   \item{%
      \textbf{Authority} Consists of the host and the port number. The host is further divided
      into the subdomain (usually www), the domain name, and the top-level domain
      (e.g., .org, .gov, .com, etc.). An example of an authority would be www.example.org:8080.
   }

   \item{%
      \textbf{Path} The host name resolves to the root directory on the web server. The path is a
      relative path from the root directory to the location of the website's contents. This will usually
      contain static pages in HTML and CSS, as well as any JS scripts, media resources, etc.
   }

   \item{%
      \textbf{Query} Queries are an optional list of parameters that trail the question mark in
      the URI and are separated by an ampersand.
   }

   \item{%
       \textbf{Fragment} The fragment is an optional sub-resource that trails the octothorp/hash (\#).
   }

   \end{itemize}

   Example URI: http://www.example.org:8080/path/to/resource?param=value\&a=5\#paragraph3
}

\item{%
   \textbf{HTTP-Version:} This will look like HTTP/x.x where x.x is the version number.
}

\item{%
   \textbf{CRLF:} CRLF stands for carriage-return followed by line-feed. Linux typically only uses
   line-feeds for separating text (e.g. \textbackslash{}n), but NTFS on Windows uses CRLF
   (\textbackslash{}r\textbackslash{}n) which is also what HTTP requests use.
}

\item{%
   \textbf{Headers:} There are a series of available headers defined by the IANA which allow the
   client and server to pass additional information alongside the Message-body. A common one that we care
   about is the accept header which tells the server about what kinds of data can be sent back.
}

\item{%
   \textbf{Message-body:} The message body is not applicable for certain HTTP methods such as GET,
   but it will apply to requests such as PUT or POST. It will usually either be XML, JSON, or HTML/text data.
   This content is effectively sent to the server alongside the request.
}

\end{itemize}

To provide a visual of an HTTP response we can use the curl command to fetch a website. You must run curl with
the verbose option (-v) to see the HTTP request and response e.g. curl -v www.neilkingdom.xyz. In my case,
this outputs the following for the HTTP request:

*   Trying 149.248.53.42:80...\\
* Connected to www.neilkingdom.xyz (149.248.53.42) port 80\\
> GET / HTTP/1.1\\
> Host: www.neilkingdom.xyz\\
> User-Agent: curl/8.4.0\\
> Accept: */*

We can see that curl made a GET request using HTTP/1.1. Since it's a get request there is no body, but we do
see some headers in the form of User-Agent and Accept. A value of */* for accept indicates that the server can
return anything it wants. HTTP responses look slightly different from requests, but follow a similar pattern:

<HTTP-Version> <Status-Code> <Reason-Phrase> <CRLF>\\
\[headers\] <CRLF>\\
<Message-body>

The only thing new here are the Status-Code and Reason-Phrase. You've surely seen an HTTP status code before
whether it was a 404 - not found, 200 - ok, 500 - internal server error, 301 - moved permanently, etc.
HTTP status codes are split into categories based on their ranges. Here are what each range represents:

\begin{itemize}

\item{Informational responses (100-199)}

\item{Successful responses (200-299)}

\item{Redirection messages (300-399)}

\item{Client error responses (400-499)}

\item{Server error responses (500-599)}

\end{itemize}

The Reason-Phrase is simply the string of text that describes the Status-Code. HTTP responses almost always
contain a Message-body. Here is the remainder of the output of the curl command that I ran previously:

< HTTP/1.1 301 Moved Permanently\\
< Server: nginx/1.22.1\\
< Date: Sat, 09 Dec 2023 17:22:09 GMT\\
< Content-Type: text/html\\
< Content-Length: 169\\
< Connection: keep-alive\\
< Location: https://www.neilkingdom.xyz/\\
<\\
<html>\\
<head><title>301 Moved Permanently</title></head>\\
<body>\\
<center><h1>301 Moved Permanently</h1></center>\\
<hr><center>nginx/1.22.1</center>\\
</body>\\
</html>

The reason I got a 301 here is actually because I didn't specify that I wanted to locate the resource using
the HTTPS protocol and curl assumed I wanted HTTP but my web server is setup to redirect to the HTTPS version.
The output is easier to read this way though, since the body is concise. We see at the top our status code
(301), followed by the reason phrase (Moved Permanently). Everything up until the HTML code are more HTTP
headers.

\section{Writing a Basic HTTP Web Server}

Of course, it is best to learn by doing, and so I'm going to provide the code for writing a basic web server in
C and then we'll go through it. This will be a repeat of the code and explanations provided in my POSIX
Programming notes.

\newpage

\begin{cblk}

#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>
#include <string.h>
#include <limits.h>
#include <stdarg.h>
#include <stdbool.h>
#include <arpa/inet.h>  // Internet socket APIs
#include <sys/socket.h> // Berkley socket APIs

#define SERVER_PORT 8080
#define CRLF "\r\n"

typedef struct sockaddr sockaddr_t;
typedef struct sockaddr_in sockaddr_in_t;

int main(void) {
    // Server socket for clients to connect to
    int listen_fd = socket(AF_INET, SOCK_STREAM, 0);
    if (listen_fd < 0) {
        fprintf(stderr, "Socket error\n");
        exit(EXIT_FAILURE);
    }

    // Server socket address (IP + port)
    sockaddr_in_t serv_addr = {
        .sin_family       = AF_INET,
        .sin_port         = htons(SERVER_PORT),
        .sin_addr.s_addr  = htonl(INADDR_ANY) // INADDR_ANY = 0.0.0.0
    };

    // Bind (i.e., map) listen_fd to the address specified in serv_addr
    if (bind(listen_fd, (sockaddr_t*)&serv_addr, sizeof(serv_addr)) < 0) {
        fprintf(stderr, "Bind error\n");
        exit(EXIT_FAILURE);
    }

    // Listen for connection requests (blocking)
    printf("Waiting for a connection at 0.0.0.0:8080\n");
    if (listen(listen_fd, 1) < 0) {
        fprintf(stderr, "Listen error\n");
        exit(EXIT_FAILURE);
    }

\end{cblk}

\newpage

\begin{cblk}

    while (true) {
        // Only accepting a single connection
        int conn_fd = accept(listen_fd, NULL, NULL);
        if (conn_fd < 0) {
            fprintf(stderr, "Accept error\n");
            exit(EXIT_FAILURE);
        }

        printf("Client connected\n");

        while (true) {
            int nb_read = 0;
            int total_read = 0;
            uint8_t recv_buf[LINE_MAX] = { 0 };
            uint8_t rply_buf[LINE_MAX] = { 0 };

            // Read request (blocking)
            while ((nb_read = read(conn_fd, recv_buf + total_read, sizeof(recv_buf) - total_read - 1)) > 0) {
                total_read += nb_read;
                recv_buf[total_read] = '\0';
                if (strstr((const char*)recv_buf, CRLF CRLF) != NULL) {
                    break;
                }
            }

            if (nb_read == 0) {
                printf("Client terminated the connection\n");
                close(conn_fd);
                break;
            } else if (nb_read == -1) {
                fprintf(stderr, "Read error\n");
                close(conn_fd);
                break;
            }

            printf("Received request:\n%s", recv_buf);

            // Send response
            const char *body = "Hello from server";
            snprintf((char*)rply_buf, sizeof(rply_buf),
                "HTTP/1.1 200 OK" CRLF
                "Content-Length: %zu" CRLF
                "Content-Type: text/plain" CRLF
                "Connection: keep-alive" CRLF
                CRLF
                "%s",
                strlen(body), body);
            write(conn_fd, rply_buf, strlen((char*)rply_buf));
        }
    }

    close(listen_fd);
    return EXIT_SUCCESS;
}

\end{cblk}

\newpage

In order to execute this code, compile using GCC e.g. gcc server.c -o server, then run the server. From a
browser, go to localhost:8080. In case you're not aware, localhost resolves to your loopback address, which
should be 127.0.0.1. As you should also know, port 80 is the standard port for HTTP and port 443 is the
standard port for HTTPS. Port numbers under 1000 are reserved and can easily conflict with other applications,
thus requiring superuser privileges to execute applications on. A common port that people use to avoid this
conflict is 8080, but you may choose any port you'd like. You should receive in the browser some text saying
"Hello from server". In the server application, you can view the HTTP requests similar to the functionality of
curl. There will likely be multiple requests because your browser will try to fetch multiple things. For
example, the first request for me specifies root (/) as the URI, followed by /favicon.ico as the URI for the
second request. ico files are used to display the icon on the tab for the current webpage and the default name
for them is favicon.

This is a lot of code, so I'll try my best to explain it bit by bit. First we create a file descriptor for the
listen socket. The server's job is to listen for connection requests, manage those connections, and parse
HTTP requests so that it can reply appropriately. A client on the other hand, actively initiates the connection
request and submits further HTTP requests to PUT, POST, GET, DELETE, etc. resources. When opening the
listen socket, we specify AF\_INET, which stands for "Address Family Internet". This means that the socket will
be an internet socket as opposed to AF\_UNIX, which would be a berkley/unix socket used for inter-process
communication (IPC). I cover berkley sockets in my POSIX programming doc. The SOCK\_STREAM option as the
second argument specifies that we want to utilize TCP, as opposed to the SOCK\_DGRAM option, which would
refer to the UDP protocol. Next, we create a sockaddr\_in struct. You'll notice that there are two typedefs
at the top of the code: one for sockaddr and the other for sockaddr\_in. The "in" version means ipv4. There
also exists a sockaddr\_in6, for ipv6. The socket API on Linux tries to remain consistent by using sockaddr\_t
as sort of the blanket type when passing arguments, sort of like an interface. We can cast from sockaddr\_in
and sockaddr\_in6 to a generic sockaddr struct because both the "in" and "in6" structs have a sockaddr struct
as their first member. Anyways, remember that a socket is effectively an IP address + a port. Once again, we
need to specify AF\_INET since we're using internet sockets, not berkley sockets, followed by the port and
address we want to listen on. We use htons() and htonl() APIs here, which stand for "host to network short"
and "host to network long", respectively. The long vesion returns a uint32\_t and the short version returns a
uint16\_t. We need to do this because the internet uses "network byte order", which is a fancy way of saying
that it uses big endian, whereas most computers use little endian (or "host byte order"). There exist reverse
APIs ntohs() and ntohl() for converting from network to host. It should also be noted that INADDR\_ANY refers
to 0.0.0.0. This is sometimes called the "default address", and it basically tells the program to listen on
all network interfaces, including the loopback address. Using this lets clients connect from either the same
computer on 127.0.0.1, or from another computer on the same LAN. Next, we bind, meaning we basically map the
address we specified to point to the socket file descriptor that we created. Next we call listen(), which is a
blocking call that waits from incomming connection requests. The first argument in the listen() function is
the socket fd, but the second argument is the number of client requests that we allow to be added to the queue
before we start simply rejecting further requests. This is very common in network programming. In this example
we're only allowing a single client connection at a time. In a more robust server, you'd have a thread pool
where each thread would correspond to its own socket and would manage the connection for a single client.
As I stated, we're only doing one connection, so we just accept() whatever the first connection is. This will
return a new socket fd that can be used for reading and writing. The reason we have a nested while-true loop
is because if the client decides to refresh the page, that will close the connection and reopen a new one, so
this allows the client to reconnect. While the connection remains established we invoke read() to get any
incomming bytes from the client. If read() returns 0, that means the client closed the connection, and -1
indicates an error. The headers portion of an HTTP request ends with two CRLFs in a row, hence why we check
if the line being received contains them using strstr(). If so, we can move on to print out the full request
and then we reply with "Hello from server". In a real HTTP server, you'd need to parse out the client request's
URI and then perform the necessary actions.

\section{Web Browsers Explained}

\section{Hostname Resolution With DNS}

We briefly touched upon DNS, but I promised we'd revisit it in further detail. A DNS server is a server that
provides DNS services (duh). The DNS server that your PC uses will initially be determined by your ISP. You are
allowed to use other DNS servers. Well known ones include Google's DNS server, which is a popular address used
to test ping (8.8.8.8), OpenDNS, Cloudflare (1.1.1.1), and more. Although you can change your default DNS, ISPs
are dumb and lame and sometimes restrict features if you don't use their designated DNS server. When you buy a
domain name through a domain name registrar, you'll have to setup some DNS records. Essentially, you'll be
putting in your domain name and the IP address that you want it to point to. The registrar then sends a request
to whichever first party is in charge of maintaining the global DNS record list to update your domain name's
address resolution.

\section{HTTPS and Web Certificates}

We know that the HTTP protocol is unsecure, and that HTTPS uses TLS/SSL to encrypt packets as they are sent over
the network. Using HTTPS is not just as simple as changing a flag though. If you're familiar with how
asymmetric end-to-end encryption (E2EE) works, a public key can be shared with any number of users, which can
only be decrypted with by computing a mathematical hash using the value of a private key. A hash function is a
one-way i.e. non-reversable algorithm that will always produce the same output for a given input. Popular
hashing algorithms for encryption include Secure Hash (SHA256) and Message Digest (MD5). Popular encryption
algorithms for actually generating the keys are Rivest,Shamir,Adleman (RSA) and Advanced Encryption Standard
(AES). In order to use HTTPS we need an SSL certificate. This certificate verifies the authenticity of the web
server that hosts the content. As you could imagine, hackers might try to exploit the end user by redirecting
their search to their server where they can perform mallicious attacks. This is the infamous "Hackers might be
trying to steal your information" message that you get sometimes. The certificate contains a public key that
can be validated by your browser to ensure that the IP address it received from the DNS server was, in fact,
your server. It used to be the case that you'd have to pay about \$100 annually to get a certificate, but now
there are projects like Let's Encrypt and Cloudflare that provide certificates for free! Certificates are
obtained from Certificate Authorities (CAs). Let's Encrypt is one such CA started by the EFF, Mozilla, and
others. There are tools like certbot that makes fetching these certificates even easier. Certificates must be
renewed monthly, but this can also be automated trivially.

\section{Static vs Dynamic Websites}

\section{Static Site Generators}

\section{Firewalls}

A firewall limits the ports that can and can't receive traffic on your machine. The default firewall on most
Linux distros is Uncomplicated Firewall (UFW). UFW is very easy to use. It will likely begin by having ports
80 and 443 enabled. We can enable ports for both IPV4 and IPV6, and also for TCP or UDP. As I mentioned before,
up until recently the web has been running on TCP, so most of the time we only enable ports on TCP, but you do
have the option if UDP is needed. Firewalls are also capable of whitelisting or blacklisting specific IP
addresses. Things get slightly more complicated when running your own server at home that you want people on
other networks to be able to access. This is because each machine manages its own ports. If we want to expose
a port to other networks, we need to do what is called port-forwarding, which is managed at the router level.
Port-forwarding allows you to map a public facing port to a specific machines port. For example, to enable
SSH, we could map the public port 2222 to the server's internal port 22. Now if we attempted to connect to the
network's public facing IP at port 2222, it would get routed to the server's port 22. We still need to have
the firewall make the server's port 22 be unblocked for it to work, however.

\section{Proxies}

A proxy is a server which acts as an intermediary party between two relays. Virtual Private Networks (VPNs) are
a good example of a proxy. If you have a VPN (well first of all, you should cancel because they are scams)
then your traffic is first being sent to their server, where they then spoof your IP address, and then forward
your request to your ISP. One of the big selling points of VPNs is that they are encrypted, but so is 90\%
of the web through HTTPS... All that you've done by paying for a VPN is brought in another party to potentially
snoop on your data, alongside your ISP, but I digress. Proxies can be used for a plethora of things since they
are just servers at the end of the day. The Tor browser uses proxies to completely anonymize you by forwarding
your request through 3 random relays i.e. proxies on the Tor network before the request exits to the internet
(this is definitely what you should be using for privacy instead of VPNs).

A reverse proxy is an application (usually ran as a daemon) that forwards client requests to a back end
application. The difference between a reverse proxy and a forward proxy is that forward
proxies accept connections from computers on a private network and forwards them to the internet, whereas
reverse proxies accept requests from the internet and send them to computers on the private network.

\section{Apache Server, NGINX, and Python Server}

We're now going to look into methods of hosting websites. Note that developers typically host their websites on
their loopback address (localhost) prior to release since it is easier to connect to your local machine and
modify code on your local machine, not to mention that fact that you don't want to be paying for a web server
while you're developing the code. Apache's HTTP server is a classic web server that pretty much used to be the
way every website was hosted. In fact, if you run a Linux distro, you probably have a directory /var/www/ which
is the root directory for the Apache server. In order to run apache, execute the httpd command (stands for
Apache HTTP Daemon) e.g. /usr/bin/httpd (the command should ran using the full path). Now, going to
localhost:80 or 127.0.0.1:80 will direct to the default Apache HTTP page. Replacing this page with your own
index.html file in /var/www/ will then serve that instead.

If you just need a super simple HTTP server quickly and easily on your local machine, python server is, in my
opinion, the way to go. If you have Python installed, just run python3 -m http.server and it will launch a new
server at 0.0.0.0:8000. Unlike Apache HTTP server which uses /var/www/ as its root directory, python server will
just use the directory that you're currently in. If no index.html page is found at the root directory, python
server sets up a local FTP server for you!

My personal favorite way of setting up actual websites has to be nginx, mostly due to its easy integration with
certbot. NGINX is a reverse proxy that you can install on your web server that will direct requests coming
from the internet to your website or web application. NGINX has a bit of a barrier to entry simply because you
need to write very verbose configuration files, but for a simple website it is relatively simple to setup.

\section{Load Balancing}

\section{Routing}

Routing is a mechanism for loading pages within a web application that has grown in popularity over the years.
For many static websites, routing is a bit redundant, since anchor tags provide us the ability to load pages
just fine. Routing comes in various forms, but generally, the way that it works is by mapping the endpoint of
the URI to a function on the server. The function will usually (though not always) fetch the appropriate page.
An endpoint is simply the resource located at the URI's path. For instance, pretend that we navigate to
https://www.example.org/get/the/endpoint. In this case, endpoint may not be an actual file on our server, but
rather, a function that will be invoked to return endpoint.html or run some other task completely. This
method of routing is known as path-based routing, as opposed to host-based, which is the conventional way of
simply returning the thing that the user requested. Another less common type of routing is header-based
routing, in which we route depending upon the headers that are present in the HTTP request. Typically, we use
custom headers to achieve this. Historically, custom HTTP headers used to start with an 'X' prefix, but this
was deprecated in 2012, so now you just have to lookup the header to see if it's proprietary or not.

\section{Dynamic Websites}

\subsection{Server-Side Applications}

\subsection{Client-Side Applications}

\subsection{Page Rehydration}

\section{Sessions}

\subsection{Cookies}

\subsection{Storage API}

\subsection{IndexedDB}

\section{History of Web Frameworks}

\begin{itemize}

\item{%
    \textbf{Django:}
}

\item{%
    \textbf{Next.js:}
}

\item{%
    \textbf{Ruby on Rails:}
}

\item{%
    \textbf{Laravel:}
}

\item{%
    \textbf{Spring:}
}

\end{itemize}

\section{Gecko, Blink, and WebKit}

\section{Web APIs}

Application Programming Interfaces (APIs), are a very important aspect of web applications nowadays. When I
reference API in programming documents, I'm typically just using the term to refer to a function belonging to
a library. In web, APIs take on a slightly different but related meaning, which is effectively that of a
contract for communications. A web API pretty much always refers to how clients communicate with the server.
A front-end application will communicate with the backend via an API. Likewise, sometimes a company will offer
an API as a paid service for other developers or businesses to utilize. For example, GitHub might offer
endpoints that we can make HTTP GET requests to to access analytics data such as information pertaining to a
particular user or repo.

\subsection{RESTful APIs}

Pretty much all web APIs are considered RESTful APIs. REST is an acronym which stands for REpresentational
State Transfer. It is not a protocol or standard, but rather a set of principles. REST has 6 guiding principles
that need to be followed in order for a web API to be considered RESTful:

\begin{enumerate}

\item Uniform Interface

\item Client-Server

\item Stateless

\item Cacheable

\item Layered System

\item Code on Demand (Optional)

\end{enumerate}

\subsection{GraphQL}

\subsection{gRPC}

\subsection{Webhooks}

\subsection{SOAP}

\subsection{JSON-RPC}

\section{Service Workers}

\section{Protobufs}

\section{WebGL}

\section{WASM and WASI}

\section{Docker}

\section{HTMX}

\section{CSS Frameworks}

\subsection{Bootstrap}

\subsection{Tailwind}

\subsection{SASS}

\end{document}
